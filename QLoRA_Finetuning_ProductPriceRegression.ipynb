{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements the fine-tuning of a base large language model (LLM) (`Llama 3.1-8B model`) for price prediction using the Amazon product description dataset. It leverages QLoRA for efficient fine-tuning and integrates with Weights & Biases for experiment tracking.\n",
        "\n",
        "Evaluation and results for this model can be found in this notebook:\n",
        "https://colab.research.google.com/drive/15rKQLoRqOleYdvyX-ReZEy_fs0i0pEKK?usp=sharing"
      ],
      "metadata": {
        "id": "F4qqBXuvfdH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMt96BGlbgrd"
      },
      "outputs": [],
      "source": [
        "# pip installs\n",
        "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
        "!pip install -q --upgrade requests==2.32.3 bitsandbytes==0.46.0 transformers==4.48.3 accelerate==1.3.0 datasets==3.2.0 peft==0.14.0 trl==0.14.0 matplotlib wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, set_seed, BitsAndBytesConfig\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "import wandb\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "H50bEN6bbxCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "\n",
        "\n",
        "# Run name for saving the model in the hub\n",
        "\n",
        "PROJECT_RUN_NAME = \"amazon-pricing-model-finetuned\"\n",
        "\n",
        "# Hyperparameters for QLoRA\n",
        "\n",
        "LORA_R = 32\n",
        "LORA_ALPHA = 64\n",
        "TARGET_MODULES = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "LORA_DROPOUT = 0.1\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "# Hyperparameters for Training\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 16 # Using A100 box\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 1e-4\n",
        "LR_SCHEDULER_TYPE = 'cosine'\n",
        "WARMUP_RATIO = 0.03\n",
        "OPTIMIZER = \"paged_adamw_32bit\"\n",
        "\n",
        "# Admin config - note that SAVE_STEPS is how often it will upload to the hub\n",
        "\n",
        "STEPS = 50\n",
        "SAVE_STEPS = 2000\n",
        "LOG_TO_WANDB = True\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "t7vQWl0Sb2_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to HuggingFace\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "2dnaOqcpdqF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log in to Weights & Biases\n",
        "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "wandb.login()\n",
        "\n",
        "# Configure Weights & Biases to record against our project\n",
        "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
        "os.environ[\"WANDB_WATCH\"] = \"gradients\""
      ],
      "metadata": {
        "id": "IwX5F3wrdwm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the data from hugging face"
      ],
      "metadata": {
        "id": "mbEjqbBJdhG7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset used in this project contains Amazon product descriptions paired with their corresponding prices. It includes detailed product information from a variety of categories, providing text descriptions and pricing data ideal for training and evaluating price prediction models based on natural language product content.\n",
        "\n",
        "https://huggingface.co/datasets/ed-donner/pricer-data/viewer/default/train?views%5B%5D=train"
      ],
      "metadata": {
        "id": "42suN4Opdd5t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_NAME = \"ed-donner/pricer-data\"\n",
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']\n",
        "\n",
        "MAX_SEQUENCE_LENGTH = 182"
      ],
      "metadata": {
        "id": "BKDhOVuxdGhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.init(project=PROJECT_NAME, name=RUN_NAME)"
      ],
      "metadata": {
        "id": "izfv8ewrd0yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Tokenizer and Model\n",
        "\n",
        "- The model is \"quantized\", reducing the precision to 4 bits."
      ],
      "metadata": {
        "id": "siw6llXBeBrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ],
      "metadata": {
        "id": "cr3A9uU9eIDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Tokenizer and the Model\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
      ],
      "metadata": {
        "id": "dal3OOqpeKRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collator\n",
        "\n",
        "It's important that we ensure during Training that we are not trying to train the model to predict the description of products; only their price.\n",
        "\n",
        "We need to tell the trainer that everything up to \"Price is $\" is there to give context to the model to predict the next token, but does not need to be learned.\n",
        "\n",
        "The trainer needs to teach the model to predict the token(s) after \"Price is $\"."
      ],
      "metadata": {
        "id": "cPzhfHDgeSdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "response_template = \"Price is $\"\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "qokoUxb5eS7_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up the configuration for Training\n",
        "\n",
        "1. A LoraConfig object with our hyperparameters for LoRA\n",
        "\n",
        "2. An SFTConfig with our overall Training parameters"
      ],
      "metadata": {
        "id": "Slcb_C62edOD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration parameters for LoRA\n",
        "\n",
        "lora_parameters = LoraConfig(\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    r=LORA_R,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=TARGET_MODULES,\n",
        ")\n",
        "\n",
        "# General configuration parameters for training\n",
        "\n",
        "train_parameters = SFTConfig(\n",
        "    output_dir=PROJECT_RUN_NAME,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=1,\n",
        "    eval_strategy=\"no\",\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    optim=OPTIMIZER,\n",
        "    save_steps=SAVE_STEPS,\n",
        "    save_total_limit=10,\n",
        "    logging_steps=STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    weight_decay=0.001,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=0.3,\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=WARMUP_RATIO,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
        "    run_name=RUN_NAME,\n",
        "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
        "    dataset_text_field=\"text\",\n",
        "    save_strategy=\"steps\",\n",
        "    hub_strategy=\"every_save\",\n",
        "    push_to_hub=True,\n",
        "    hub_model_id=HUB_MODEL_NAME,\n",
        "    hub_private_repo=True\n",
        ")\n",
        "\n",
        "fine_tuning = SFTTrainer(\n",
        "    model=base_model,\n",
        "    train_dataset=train,\n",
        "    peft_config=lora_parameters,\n",
        "    args=train_parameters,\n",
        "    data_collator=collator\n",
        "  )"
      ],
      "metadata": {
        "id": "WA65pGJFekJ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running the finetuning"
      ],
      "metadata": {
        "id": "L-HWJiJ9e3Gk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune!\n",
        "fine_tuning.train()\n",
        "\n",
        "# Push our fine-tuned model to Hugging Face\n",
        "fine_tuning.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
        "print(f\"Saved to the hub: {PROJECT_RUN_NAME}\")"
      ],
      "metadata": {
        "id": "hdWmJWQde8PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOG_TO_WANDB:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "y78CiUdBfCks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}